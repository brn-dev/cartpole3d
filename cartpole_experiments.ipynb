{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "\n",
    "from dm_control import mjcf\n",
    "from dm_control import viewer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as nn_init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from cartpole3d import CartPole3D\n",
    "from rendering import display_video\n",
    "\n",
    "from src.networks.self_normalizing_fnn import SelfNormalizingFNN\n",
    "from src.policies.state_model_fnn_policy_2 import StateModelFnnPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 613413384\n",
      "Model has 11322 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████████▋                                                                                                                                                                                                  | 33/500 [00:24<05:41,  1.37it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "seed = time.time_ns() % (2**32)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(f'seed is {seed}')\n",
    "\n",
    "def step_reward(time, action, state, prev_state):\n",
    "    reward = 0.0\n",
    "\n",
    "    reward += 0.15\n",
    "    \n",
    "    reward += -0.0 * time\n",
    "\n",
    "    reward += -0.0 * np.linalg.norm(state[0:2])  # slide distance\n",
    "    reward += -0.0 * np.linalg.norm(state[2:4])  # hinge distance\n",
    "    \n",
    "    reward += -0.1 * (np.linalg.norm(state[0:2]) - np.linalg.norm(prev_state[0:2]))\n",
    "    reward += -1.0 * (np.linalg.norm(state[2:4]) - np.linalg.norm(prev_state[2:4]))\n",
    "    \n",
    "    return reward\n",
    "\n",
    "env = CartPole3D(\n",
    "    nr_movement_dimensions=2,\n",
    "    cart_size=0.25,\n",
    "    force_magnitude=5000,\n",
    "    physics_steps_per_step=1,\n",
    "    reset_randomization_magnitude=0.1,\n",
    "    slide_range=1.5,\n",
    "    hinge_range=1,\n",
    "    time_limit=20.0,\n",
    "    step_reward_function=step_reward,\n",
    "    out_ouf_range_reward_function=lambda time, action, state: 0,\n",
    "    time_limit_reward_function=lambda time, action, state: 100,\n",
    ")\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "action_size = env.action_space.shape[0]\n",
    "obs_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    saved_actions = []\n",
    "    saved_log_probs = []\n",
    "    saved_state_preds = []\n",
    "    saved_state_targets = []\n",
    "    rewards = []\n",
    "    \n",
    "    action_dist_sd = 0.15\n",
    "        \n",
    "    def stringify_np_array(arr):\n",
    "        return \", \".join([f\"{x:> 2.4f}\" for x in arr])\n",
    "    \n",
    "    def init_policy():\n",
    "        return StateModelFnnPolicy(\n",
    "            action_size=action_size,\n",
    "            state_size=obs_size,\n",
    "            hidden_sizes=[\n",
    "                64,\n",
    "                64,\n",
    "                64,\n",
    "                32\n",
    "            ],\n",
    "        )    \n",
    "    \n",
    "    def select_action(mean_predictions, i_episode):    \n",
    "        action_dist = torch.distributions.Normal(\n",
    "            mean_predictions, \n",
    "            action_dist_sd # / (1 + i_episode / 30)\n",
    "        )\n",
    "        action = action_dist.sample()\n",
    "        saved_log_probs.append(action_dist.log_prob(action))\n",
    "\n",
    "        return action.detach().cpu().numpy()\n",
    "    \n",
    "    def finish_episode(policy, optimizer):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = deque()\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.appendleft(R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy_loss = 5.0 * torch.cat(policy_loss).mean()\n",
    "\n",
    "        if include_state_pred:\n",
    "            state_pred_loss = 1.0 * F.mse_loss(torch.stack(saved_state_preds), torch.stack(saved_state_targets))\n",
    "            total_loss = policy_loss + state_pred_loss\n",
    "        else:\n",
    "            total_loss = policy_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del rewards[:]\n",
    "        del saved_log_probs[:]\n",
    "        del saved_state_preds[:]\n",
    "        del saved_state_targets[:]\n",
    "        del saved_actions[:]\n",
    "        \n",
    "        return returns\n",
    "        \n",
    "    print(f'Model has {sum(p.numel() for p in init_policy().parameters() if p.requires_grad)} parameters')\n",
    "\n",
    "    for gamma in (0.0, 0.5, 0.99):\n",
    "        for include_state_pred in (True, False):\n",
    "            with open(f'experiments_logs/cartpole_experiment_{gamma}_{include_state_pred}_{int(time.time())}.csv', mode='w', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',')\n",
    "                csv_writer.writerow(['gamma', 'include_state_pred', 'best_timestep', 'i_episode'])\n",
    "                for i_trial in range(100):                    \n",
    "                    trial_best_timestep = 0\n",
    "                    \n",
    "                    policy = init_policy()\n",
    "                    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "            \n",
    "                    for i_episode in tqdm(range(1, 501)):\n",
    "                        state, _ = env.reset()\n",
    "                        action_pred_log = []\n",
    "                        info = {}\n",
    "            \n",
    "                        for timestep in range(0, 1000000):  # Don't infinite loop while learning\n",
    "                            action_pred, state_pred = policy.forward(torch.tensor(state).float())\n",
    "                            \n",
    "                            action = select_action(action_pred, i_episode)\n",
    "                            \n",
    "                            state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "                            saved_state_preds.append(state_pred)\n",
    "                            saved_state_targets.append(torch.tensor(state).float())\n",
    "                            saved_actions.append(action_pred)\n",
    "                            \n",
    "                            action_pred_log.append(action_pred.detach().cpu().numpy())\n",
    "                            rewards.append(reward)\n",
    "                            \n",
    "                            if done:\n",
    "                                break\n",
    "            \n",
    "                        if timestep > trial_best_timestep:\n",
    "                            trial_best_timestep = timestep\n",
    "            \n",
    "                        if truncated:\n",
    "                            break\n",
    "                \n",
    "                        finish_episode(policy, optimizer)\n",
    "    \n",
    "                    log_entries = [gamma, include_state_pred, trial_best_timestep, i_episode]\n",
    "                    csv_writer.writerow(log_entries)\n",
    "                    print(', '.join([str(x) for x in log_entries]))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
