{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "\n",
    "from dm_control import mjcf\n",
    "from dm_control import viewer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as nn_init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from cartpole3d import CartPole3D\n",
    "from rendering import display_video\n",
    "\n",
    "from modules.self_normalizing_fnn import SelfNormalizingFNN\n",
    "from modules.policies.state_model_fnn_policy_2 import StateModelFnnPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 2735177452\n",
      "Model has 11306 parameters\n",
      "35\n",
      "b\n",
      "44\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "50\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 163\u001b[0m\n\u001b[0;32m    161\u001b[0m                     csv_writer\u001b[38;5;241m.\u001b[39mwriterow(log_entries)\n\u001b[0;32m    162\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m log_entries]))\n\u001b[1;32m--> 163\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 158\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m truncated:\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m log_entries \u001b[38;5;241m=\u001b[39m [gamma, include_state_pred, trial_best_timestep, i_episode]\n\u001b[0;32m    161\u001b[0m csv_writer\u001b[38;5;241m.\u001b[39mwriterow(log_entries)\n",
      "Cell \u001b[1;32mIn[15], line 105\u001b[0m, in \u001b[0;36mmain.<locals>.finish_episode\u001b[1;34m(policy, optimizer)\u001b[0m\n\u001b[0;32m    102\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m policy_loss\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m rewards[:]\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\_tensor.py:483\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[1;32m-> 1560\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1562\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "seed = time.time_ns() % (2**32)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(f'seed is {seed}')\n",
    "\n",
    "def step_reward(time, action, state, prev_state):\n",
    "    reward = 0.0\n",
    "\n",
    "    reward += 0.15\n",
    "    \n",
    "    reward += -0.0 * time\n",
    "\n",
    "    reward += -0.0 * np.linalg.norm(state[0:2])  # slide distance\n",
    "    reward += -0.0 * np.linalg.norm(state[2:4])  # hinge distance\n",
    "    \n",
    "    reward += -0.1 * (np.linalg.norm(state[0:2]) - np.linalg.norm(prev_state[0:2]))\n",
    "    reward += -1.0 * (np.linalg.norm(state[2:4]) - np.linalg.norm(prev_state[2:4]))\n",
    "    \n",
    "    return reward\n",
    "\n",
    "env = CartPole3D(\n",
    "    nr_movement_dimensions=2,\n",
    "    cart_size=0.25,\n",
    "    force_magnitude=5000,\n",
    "    physics_steps_per_step=1,\n",
    "    reset_randomization_magnitude=0.1,\n",
    "    slide_range=1.5,\n",
    "    hinge_range=1,\n",
    "    time_limit=20.0,\n",
    "    step_reward_function=step_reward,\n",
    "    out_ouf_range_reward_function=lambda time, action, state: 0,\n",
    "    time_limit_reward_function=lambda time, action, state: 100,\n",
    ")\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "action_size = env.action_space.shape[0]\n",
    "obs_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    saved_actions = []\n",
    "    saved_log_probs = []\n",
    "    saved_state_preds = []\n",
    "    saved_state_targets = []\n",
    "    rewards = []\n",
    "    \n",
    "    action_dist_sd = 0.15\n",
    "        \n",
    "    def stringify_np_array(arr):\n",
    "        return \", \".join([f\"{x:> 2.4f}\" for x in arr])\n",
    "    \n",
    "    def init_policy():\n",
    "        return StateModelFnnPolicy(\n",
    "            action_size=action_size,\n",
    "            state_size=obs_size,\n",
    "            hidden_sizes=[\n",
    "                64,\n",
    "                64,\n",
    "                64,\n",
    "                32\n",
    "            ],\n",
    "        )    \n",
    "    \n",
    "    def select_action(mean_predictions, i_episode):    \n",
    "        action_dist = torch.distributions.Normal(\n",
    "            mean_predictions, \n",
    "            action_dist_sd # / (1 + i_episode / 30)\n",
    "        )\n",
    "        action = action_dist.sample()\n",
    "        saved_log_probs.append(action_dist.log_prob(action))\n",
    "\n",
    "        return action.detach().cpu().numpy()\n",
    "    \n",
    "    def finish_episode(policy, optimizer):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = deque()\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.appendleft(R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy_loss = 5.0 * torch.cat(policy_loss).mean()\n",
    "\n",
    "        if include_state_pred:\n",
    "            state_pred_loss = 1.0 * F.mse_loss(torch.stack(saved_state_preds), torch.stack(saved_state_targets))\n",
    "            total_loss = policy_loss + state_pred_loss\n",
    "        else:\n",
    "            total_loss = policy_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del rewards[:]\n",
    "        del saved_log_probs[:]\n",
    "        del saved_state_preds[:]\n",
    "        del saved_state_targets[:]\n",
    "        del saved_actions[:]\n",
    "        \n",
    "        return returns\n",
    "        \n",
    "    print(f'Model has {sum(p.numel() for p in init_policy().parameters() if p.requires_grad)} parameters')\n",
    "\n",
    "    for gamma in (0.0, 0.5, 0.99):\n",
    "        for include_state_pred in (True, False):\n",
    "            with open(f'experiments_logs/cartpole_experiment_{gamma}_{include_state_pred}_{int(time.time())}.csv', mode='w', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',')\n",
    "                csv_writer.writerow(['gamma', 'include_state_pred', 'best_timestep', 'i_episode'])\n",
    "                for i_trial in range(100):                    \n",
    "                    trial_best_timestep = 0\n",
    "                    \n",
    "                    policy = init_policy()\n",
    "                    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "            \n",
    "                    for i_episode in range(1, 501):\n",
    "                        state, _ = env.reset()\n",
    "                        action_pred_log = []\n",
    "                        info = {}\n",
    "            \n",
    "                        for timestep in range(0, 1000000):  # Don't infinite loop while learning\n",
    "                            action_pred, state_pred = policy.forward(torch.tensor(state).float())\n",
    "                            \n",
    "                            action = select_action(action_pred, i_episode)\n",
    "                            \n",
    "                            state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "                            saved_state_preds.append(state_pred)\n",
    "                            saved_state_targets.append(torch.tensor(state).float())\n",
    "                            saved_actions.append(action_pred)\n",
    "                            \n",
    "                            action_pred_log.append(action_pred.detach().cpu().numpy())\n",
    "                            rewards.append(reward)\n",
    "                            \n",
    "                            if done:\n",
    "                                break\n",
    "            \n",
    "                        if timestep > trial_best_timestep:\n",
    "                            trial_best_timestep = timestep\n",
    "            \n",
    "                        if truncated:\n",
    "                            break\n",
    "                \n",
    "                        finish_episode(policy, optimizer)\n",
    "    \n",
    "                    log_entries = [gamma, include_state_pred, trial_best_timestep, i_episode]\n",
    "                    csv_writer.writerow(log_entries)\n",
    "                    print(', '.join([str(x) for x in log_entries]))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
